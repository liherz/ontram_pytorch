{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Test pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from ontram_pytorch import OntramModel\n",
    "from ontram_pytorch import fit_ontram, predict_ontram, classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder /home/hezo/ontram_pytorch/checkpoints_cifar10/ already exists.\n"
     ]
    }
   ],
   "source": [
    "DIR = '/home/hezo/'\n",
    "OUTPUT_DIR = DIR + 'ontram_pytorch/checkpoints_cifar10/'\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "    print(f\"Created folder {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"Folder {OUTPUT_DIR} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Image batch shape: torch.Size([32, 3, 32, 32])\n",
      "Label batch shape: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "# Define the transformations to apply to each image.\n",
    "# For example, convert images to tensors and normalize them.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize RGB channels\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# one hot encode labels for ontram\n",
    "def one_hot_encode_labels(labels, num_classes=10):\n",
    "    return F.one_hot(labels, num_classes=num_classes).float()\n",
    "\n",
    "# Custom collate function to apply one-hot encoding\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)  # Unpack images and labels\n",
    "    images = torch.stack(images, dim=0)  # Stack images into a single tensor\n",
    "    labels = torch.tensor(labels)  # Convert labels to tensor\n",
    "    labels = one_hot_encode_labels(labels)  # One-hot encode labels\n",
    "    return images, labels\n",
    "\n",
    "# consider only a subset\n",
    "subset_indices = list(range(20000))\n",
    "train_dataset_sub = Subset(train_dataset, subset_indices)\n",
    "subset_indices = list(range(2000))\n",
    "test_dataset_sub = Subset(test_dataset, subset_indices)\n",
    "\n",
    "# Create train and test DataLoaders with the custom collate function\n",
    "train_loader = DataLoader(train_dataset_sub, batch_size=32, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset_sub, batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Label batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9607843..0.99215686].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_pt, label = train_dataset[1]\n",
    "img = img_pt.numpy().transpose((1, 2, 0))\n",
    "\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.title(f\"Label: {train_dataset.classes[label]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes: \", train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "device = 'cuda'\n",
    "epochs = 20\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 10, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4) \n",
    "        x = self.relu(self.fc1(x)) \n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc2(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "model = SimpleCNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7078850456237793\n",
      "Epoch 2, Loss: 1.3427010618209838\n",
      "Epoch 3, Loss: 1.167658680343628\n",
      "Epoch 4, Loss: 1.0196236207008362\n",
      "Epoch 5, Loss: 0.9211402674674988\n",
      "Epoch 6, Loss: 0.8141905107498169\n",
      "Epoch 7, Loss: 0.7290066030979157\n",
      "Epoch 8, Loss: 0.6610491582870484\n",
      "Epoch 9, Loss: 0.5990523226737976\n",
      "Epoch 10, Loss: 0.5335206910848618\n",
      "Epoch 11, Loss: 0.4817766573667526\n",
      "Epoch 12, Loss: 0.43977753870487213\n",
      "Epoch 13, Loss: 0.39883075156211856\n",
      "Epoch 14, Loss: 0.36704229617118833\n",
      "Epoch 15, Loss: 0.33183709685206414\n",
      "Epoch 16, Loss: 0.3085206061542034\n",
      "Epoch 17, Loss: 0.2892671183884144\n",
      "Epoch 18, Loss: 0.26957514313459396\n",
      "Epoch 19, Loss: 0.2505058567225933\n",
      "Epoch 20, Loss: 0.24659470533132552\n",
      "Test Accuracy: 69.35%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # print(\"Batch loss: \", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, true = torch.max(labels, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == true).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ontram implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder /home/hezo/ontram_pytorch/checkpoints_cifar10/csb/ already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create folder for model\n",
    "MODEL_DIR = OUTPUT_DIR + 'csb/'\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    print(f\"Created folder {MODEL_DIR}\")\n",
    "else:\n",
    "    print(f\"Folder {MODEL_DIR} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        # Output layer: Change number of classes to C-1=9 and bias=False\n",
    "        self.fc2 = nn.Linear(256, 9, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4) \n",
    "        x = self.relu(self.fc1(x)) \n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc2(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with GPU support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 2.1765, Validation Loss: 0.0000\n",
      "Epoch 2/20, Train Loss: 1.8134, Validation Loss: 0.0000\n",
      "Epoch 3/20, Train Loss: 1.6828, Validation Loss: 0.0000\n",
      "Epoch 4/20, Train Loss: 1.5964, Validation Loss: 0.0000\n",
      "Epoch 5/20, Train Loss: 1.5353, Validation Loss: 0.0000\n",
      "Epoch 6/20, Train Loss: 1.4936, Validation Loss: 0.0000\n",
      "Epoch 7/20, Train Loss: 1.4505, Validation Loss: 0.0000\n",
      "Epoch 8/20, Train Loss: 1.4074, Validation Loss: 0.0000\n",
      "Epoch 9/20, Train Loss: 1.3744, Validation Loss: 0.0000\n",
      "Epoch 10/20, Train Loss: 1.3330, Validation Loss: 0.0000\n",
      "Epoch 11/20, Train Loss: 1.3045, Validation Loss: 0.0000\n",
      "Epoch 12/20, Train Loss: 1.2766, Validation Loss: 0.0000\n",
      "Epoch 13/20, Train Loss: 1.2425, Validation Loss: 0.0000\n",
      "Epoch 14/20, Train Loss: 1.2158, Validation Loss: 0.0000\n",
      "Epoch 15/20, Train Loss: 1.1858, Validation Loss: 0.0000\n",
      "Epoch 16/20, Train Loss: 1.1598, Validation Loss: 0.0000\n",
      "Epoch 17/20, Train Loss: 1.1342, Validation Loss: 0.0000\n",
      "Epoch 18/20, Train Loss: 1.1113, Validation Loss: 0.0000\n",
      "Epoch 19/20, Train Loss: 1.0866, Validation Loss: 0.0000\n",
      "Epoch 20/20, Train Loss: 1.0591, Validation Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [2.1764887098312378,\n",
       "  1.8133694383621215,\n",
       "  1.6827516424179076,\n",
       "  1.596358761405945,\n",
       "  1.5353078927993775,\n",
       "  1.4936164836883545,\n",
       "  1.4505273359298707,\n",
       "  1.4073696729660035,\n",
       "  1.374352462863922,\n",
       "  1.3329762325286865,\n",
       "  1.304539942073822,\n",
       "  1.276573459625244,\n",
       "  1.24249857711792,\n",
       "  1.215767066192627,\n",
       "  1.1858263716697692,\n",
       "  1.1598230562210083,\n",
       "  1.1342203300476075,\n",
       "  1.1112556621551513,\n",
       "  1.0865833562850953,\n",
       "  1.0591016373634339],\n",
       " 'val_loss': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontram_model = OntramModel(SimpleCNN1())\n",
    "\n",
    "# Slower in training (makes sense, we have no ordinal outcome here), learing rate has to be lower\n",
    "fit_ontram(ontram_model, train_loader, \n",
    "           optimizer=torch.optim.Adam(ontram_model.parameters(), lr=0.0001), \n",
    "           epochs=epochs, checkpoint_path=MODEL_DIR, si=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_ontram(ontram_model, test_loader, output='all', si=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = []\n",
    "for images, labels in test_loader:  # Assuming (images, labels) format\n",
    "    y_test.extend(labels.numpy())  # Convert to numpy for easy handling\n",
    "y_test = np.array(y_test)  # Convert to numpy array (optional)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6315}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_metrics(results, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
